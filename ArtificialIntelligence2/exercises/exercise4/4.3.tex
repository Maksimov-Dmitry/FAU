\documentclass{homework}
\usepackage{diagbox}
\usepackage{nicematrix}

\title{Assignment4: Information Value and Markov Basics}
\author{
  Dmitrii, Maksimov\\
  \texttt{dmitrii.maksimov@fau.de} \\
  \texttt{ko65beyp}
  \and
  Ilia, Dudnik\\
  \texttt{ilia.dudnik@fau.de}\\
  \texttt{ex69ahum}
  \and
  Aleksandr, Korneev\\
  \texttt{aleksandr.korneev@fau.de}\\
  \texttt{uw44ylyz}
}
\begin{document}

\maketitle

\exercise[4.3 (Moody HMM)]
Consider the Markov process from Ex. 4.2 about the roommate Moody (which
in particular gives the concrete probabilities needed below). We have already modeled it as an HMM with state variables $M_d$ and evidence variables $L_d$.

Because the transition model is first-order and stationary, we can collect the conditional probabilities for the state transitions into a matrix $T_{ij}=P(M_d=x_j|M_{d-1})$, where $x_i, x_j$ are two states (i.e., two possible values of the state variable). We use $S$ for the number of states, and $T$ is an $S\times S$ matrix.

Because the sensor model is stationary and has the sensor Markov property, we can collect the conditional probabilities for the observations into a matrix $O_{ij}=P(L_d=y_i|M_d=x_j)$ is a state and the $y_i$ are the possible observations. If there are $N$ possible observations, this is an $N \times S$ matrix. For a fixed observation $e$ the diagonal $S\times S$ matrices $O_e$ from the lecture notes are obtained from the rows of this matrix.
\begin{enumerate}
	\item Clarify the modeling as an HMM. Concretely:
	\begin{enumerate}
		\item What is $S$? Give the transition matrix $T$.
		
		$S$ is the possible observations of $M_d$, $S\in \{h,s\}$.
		$T = $
		  \begin{tabular}{ | c | c | c| }
		    \hline
		    \diagbox[width=4em]{\footnotesize{$M_{d-1}$}}{\footnotesize{$M_d$}} & h & s \\ \hline
		    h & 0.85 & 0.15 \\ \hline
		    s & 0.3 & 0.7 \\
		    \hline
		  \end{tabular}
		\item What is $N$? Give the sensor matrix $O$.

		$N$ is the possible observations of $L_d$, $N\in \{j,m\}$.  
		$O = $
		  \begin{tabular}{ | c | c | c| }
		    \hline
		    \diagbox[width=3em]{\footnotesize{$L_d$}}{\footnotesize{$M_d$}} & h & s \\ \hline
		    j & 0.7 & 0.4 \\ \hline
		    m & 0.3 & 0.6 \\
		    \hline
		  \end{tabular}
	\end{enumerate}
	\item Now consider a fixed sequence $L_1 = e_1,L_2 = e_2$ of observations that we have made for two days. Concretely, you heard Moody play metal on day $d = 1$ and jazz on day $d = 2$.
	\begin{enumerate}
	\item Give the diagonal sensor matrices $O_1$ and $O_2$ corresponding to the observation at $d = 1$ and $d = 2$.
	$$
	O_1 = \begin{pNiceMatrix}
	0.3 & 0\\
	0 & 0.6
	\end{pNiceMatrix},
	O_2 = \begin{pNiceMatrix}
	0.7 & 0\\
	0 & 0.4
	\end{pNiceMatrix}
	$$
	\item You are not sure what kind of mood your flatmate was in on day $d = 0$, but it was either good or bad with equal probability. The HMM algorithm for filtering and smoothing uses compact matrix/vector equation to compute $f$ and $b$. Use those equation to determine the probability distribution of Moody's mood on day $d = 1$.
	\begin{itemize}
		\item filtering

		$f_{1:t+1} = \alpha O_{t+1}T^Tf_{1:t} \Rightarrow f_{1:1} = \alpha O_{1}T^Tf_{0}$,  where $f_{0} =
		\begin{pNiceMatrix}
		0.5\\
		0.5
		\end{pNiceMatrix}$.
		\newline Hence, $f_{1:1} = \alpha \cdot \langle0.1725, 0.255\rangle=\langle 0.404, 0.596\rangle \Rightarrow \newline P(M_1 = h) = 0.404, P(M_1 = s) = 0.596$
		\item smoothing

		$P(X_k|e_{1:t}) = \alpha \cdot f_{1:k}\cdot b_{k+1:t}\Rightarrow P(X_1|e_{1:2})=\alpha \cdot f_{1:1}\cdot b_{2:2},$ \newline where $b_{2:2} = TO_2b_{3:2}, b_{3:2} = 
		\begin{pNiceMatrix}
		1\\
		1
		\end{pNiceMatrix}$.\newline
	Hence, $P(X_1|e_{1:2}) = \alpha \cdot \langle 0.404, 0.596\rangle \cdot \langle 0.655, 0.49\rangle = \langle 0.475, 0.525\rangle \Rightarrow \newline P(M_1 = h) = 0.475, P(M_1 = s) = 0.525$
	\end{itemize}
	\end{enumerate}
\end{enumerate}
\end{document}