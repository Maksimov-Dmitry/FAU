{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 2. Data Analysis & Preprocessing\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Data Analysis & Preprocessing*. The series itself includes practical exercises for lectures *3. Getting to Know Your Data* and *4. Data Preprocessing*. \n",
    "\n",
    "Exercises for data analysis and preprocessing is divided into three parts in total, namely:\n",
    "\n",
    "- Part One: Getting to Know Your Data\n",
    "- Part Two: Preprocessing - Data Cleaning & Data Integration\n",
    "- Part Three: Preprocessing - Data Reduction, Data Transformation & Data Discretization\n",
    "\n",
    "Recall that we have two exercise groups. Depending on how each group progresses, some parts of these exercises may not be discussed in its entirety. If questions arise, ask them in your study group or in our StudOn forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part Three: Preprocessing - Data Reduction, Data Transformation & Data Discretization\n",
    "\n",
    "In this part you will apply the theoretical knowledge gained in the second part of the lecture *4. Data Preprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import os\n",
    "import urllib.request\n",
    "import sklearn.decomposition\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Build path to database\n",
    "database_path = os.path.join(dataset_folder, \"adventure-works.db\")\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    database_path,\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the clean DataFrame(s)\n",
    "# Order DataFrame\n",
    "order_df = pd.read_sql_query(\n",
    "    \"SELECT p.ProductID,p.Name,p.ProductNumber,p.MakeFlag,p.FinishedGoodsFlag,p.Color,p.SafetyStockLevel,\"\n",
    "    \"p.ReorderPoint,p.StandardCost,p.ListPrice,p.Size,p.SizeUnitMeasureCode,p.WeightUnitMeasureCode,p.Weight,\"\n",
    "    \"p.DaysToManufacture,p.ProductLine,p.Class,p.Style,p.ProductSubcategoryID,p.ProductModelID,p.SellStartDate,\"\n",
    "    \"p.SellEndDate,p.DiscontinuedDate,d.PurchaseOrderID,d.PurchaseOrderDetailID,d.DueDate,d.OrderQty,d.ProductID,\"\n",
    "    \"d.UnitPrice,d.ReceivedQty,d.RejectedQty,h.RevisionNumber,h.Status,h.EmployeeID,h.VendorID,h.ShipMethodID,\"\n",
    "    \"h.OrderDate,h.ShipDate,h.SubTotal,h.TaxAmt,h.Freight,h.TotalDue,e.NationalIDNumber,e.LoginID,e.OrganizationNode,\"\n",
    "    \"e.JobTitle,e.BirthDate,e.MaritalStatus,e.Gender,e.HireDate,e.SalariedFlag,e.VacationHours,e.SickLeaveHours,\"\n",
    "    \"e.CurrentFlag,r.PersonType,r.NameStyle,r.Title,r.FirstName,r.MiddleName,r.LastName,r.Suffix,r.EmailPromotion,\"\n",
    "    \"r.AdditionalContactInfo,r.Demographics \"\n",
    "    \"FROM Product p \"\n",
    "    \"JOIN PurchaseOrderDetail d ON p.ProductID = d.ProductID \"\n",
    "    \"JOIN PurchaseOrderHeader h ON d.PurchaseOrderID = h.PurchaseOrderID \"\n",
    "    \"JOIN Employee e ON h.EmployeeID = e.BusinessEntityID \"\n",
    "    \"JOIN Person r ON e.BusinessEntityID = r.BusinessEntityID\",\n",
    "    connection,\n",
    "    index_col=\"PurchaseOrderDetailID\",\n",
    ")\n",
    "\n",
    "# CurrencyRate DataFrame\n",
    "currency_rate_df = pd.read_sql_query(\n",
    "    \"SELECT STRFTIME('%Y-%m-%d', CurrencyRateDate) AS CurrencyRateDate,AverageRate,EndOfDayRate \"\n",
    "    \"FROM CurrencyRate \"\n",
    "    \"WHERE FromCurrencyCode='USD' AND ToCurrencyCode='EUR'\",\n",
    "    connection,\n",
    "    index_col=\"CurrencyRateDate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Normalization\n",
    "\n",
    "One method introduced in the lecture and frequently used in Data Science is normalization. In order to apply this practically, we will first take a look at a part of the `order_df` already known from Part One. More precisely, we are looking at some numeric attributes from `order_df`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Display the head of the attributes `SubTotal`, `Freight` and `OrderQty` from `order_df`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the head of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Display the minimum, maximum, mean, and standard deviation of `SubTotal`, `Freight` and `OrderQty`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean, and standard deviation of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "As can be seen, the three attributes differ significantly with respect to their distribution of values posing a hindrance for some knowledge discovery tasks. To alleviate this problem, normalization is employed to scale attribute values to a smaller range.\n",
    "\n",
    "In the lecture you were introduced to three different variants of normalization: min-max normalization, z-score normalization, and normalization by decimal scaling.\n",
    "\n",
    "Below you can see the implementation of one of the normalization methods for the attributes `SubTotal`, `Freight` and `OrderQty`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute a \"mystery\" normalization in a \"pythonic\" way\n",
    "def mystery_normalization(df):\n",
    "    # Compute the normalization with a sinple formula\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "\n",
    "# Apply previously defined function\n",
    "mystery_normalization_df = mystery_normalization(\n",
    "    order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "mystery_normalization_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Determine whether the above function `mystery_normalization` is an implementation of min-max normalization, z-score normalization, or normalization by decimal scaling.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "<b>The function is an implementation of:</b>\n",
    "1. [ ] Min-max normalization (for the interval [0, 1])\n",
    "2. [ ] Z-score normalization\n",
    "3. [ ] Normalization by decimal scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Implement a function for each of the three normalization methods you got to know. (You may, of course, reuse the above code when you work on the corresponding function).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement min-max normalization for the interval [0, 1]\n",
    "def min_max_normalization(df):\n",
    "    # ...\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply min-max normalization\n",
    "min_max_df = min_max_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "min_max_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean, and standard deviation values of min_max_df\n",
    "min_max_df.agg([min, max, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement z-score normalization\n",
    "def z_score_normalization(df):\n",
    "    # ...\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply z-score normalization\n",
    "z_score_df = z_score_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "z_score_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean and standard deviation values of z_score_df\n",
    "z_score_df.agg([min, max, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement normalization by decimal scaling\n",
    "def normalization_by_decimal_scaling(df):\n",
    "    # ...\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply normalization_by_decimal_scaling\n",
    "decimal_scaling_df = normalization_by_decimal_scaling(\n",
    "    order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "decimal_scaling_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean and standard deviation values of decimal_scaling_df\n",
    "decimal_scaling_df.agg([min, max, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Note that each normalization results in different scaled values. It is therefore important to consider which normalization method best serves your purpose. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Consider when the various normalization methods presented might be beneficial.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Discretization\n",
    "\n",
    "Another commonly used method is discretization. This is used to convert a continuous values to discrete values. This method is best demonstrated on a continuous attribute which is why we take a look at the `currency_rate_df` which represents exchange rates from USD to EUR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Print the head of currency_rate_df\n",
    "currency_rate_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw the progression of the two rates over time\n",
    "currency_rate_df.plot(subplots=True)\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Sometimes it is desirable to divide attribute values into *groups* resulting in fewer values that are easier to handle. Graphically, we have already applied a method in Part One that does just that: Histogram plot.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Draw a histogram with five bins for the attributes `AverageRate` and `EndOfDayRate`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw a histogram for AverageRate and EndOfDayRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Although histogram analysis divides individual values into different groups and thus discretizes data, it completely ignores the temporal aspect. More specifically, both attributes `AverageRate` and `EndOfDayRate` constitute time series, i. e. data is indexed by time, and binning data values ignores this time dependency. \n",
    "\n",
    "Alternatively, data can be binned and thus discretize by maintaining the temporal nature by partitioning the data values to equal-width bins. This is possible in pandas via the fucntion `cut`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use `cut()` to distribute the attribute values of `AverageRate` into five bins with equal width. (Help: [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.cut.html))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute the attribute values of AverageRate into bins with equal width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Find out how to interpret the interval notation used in the new attribute values and what boundaries each of the five bins has. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Besides equal-width partioning, pandas also supports equal-depth partioning where each bin contains approximately the same *number of values*. This is done with the function `qcut`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use `qcut()` to arrange attribute values of AverageRate into five bins with equal depth. (Help: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.qcut.html\">pandas documentation</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute attribute values of AverageRate into bins with equal depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The disadvantage of this \"pure\" partitioning is that the attribute values are no longer purely numerical attributes and thus further processing is only possible to a limited extent. Therefore, one or two representative values are often selected for each bin, to which all values within the bin are smoothed.\n",
    "\n",
    "In the lecture the variant smoothing by bin means was presented, which we will now have a look at here. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Implement a method for smoothing by bin means with equal width partitioning by completing the following program skeleton. You may use `cut()`, but you don't have to. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a method for smoothing by bin means with equal width partitioning\n",
    "def smoothing_by_bin_means_with_equal_width_part(df, bins=10):\n",
    "    # Create a copy to avoid overriding original content\n",
    "    smoothed = df.copy()\n",
    "\n",
    "    # Smooth every column\n",
    "    for column in smoothed.columns:\n",
    "\n",
    "        # ...\n",
    "        continue\n",
    "\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "# Apply smoothing_by_bin_means\n",
    "currency_rate_smoothed_df = smoothing_by_bin_means_with_equal_width_part(\n",
    "    currency_rate_df, 5\n",
    ")\n",
    "currency_rate_smoothed_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw the progression of the two rates over time\n",
    "currency_rate_smoothed_df.plot(subplots=True)\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In addition to histogram analysis and binning, there are other methods of discretization. One of them - clustering - will be covered later in the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Data Reduction\n",
    "\n",
    "Another important part of data preprocessing is to reduce the dimentionality of data. One focus in the lecture was Principal Component Analysis (PCA), which we now utilize.\n",
    "\n",
    "Subject of analysis is still the `currency_rate_df` where `AverageRate` and `EndOfDayRate` are visibly the same. It is, therefore, expected that a large part of the redundancy can be eliminated by PCA.\n",
    "\n",
    "In order to generate a deep understanding on PCA, you will first apply the steps presented in the lecture before resorting to a library function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Standardize the `currency_rate_df` to ensure that all attributes are included in the analysis to the same extent. (Hint: You may use one of your previous defined functions) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Standardize currency_rate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Calculate the covariance matrix for the standardized `DataFrame`. (Help: [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cov.html))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Calculate the associated eigenvalues and eigenvectors. (Help: [Numpy documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html)) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the associated eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Calculate the percentage of information per eigenvector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of information per eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Select the feature matrix so that the transformation preserves at least 80% of the information contained in the standardized `currency_rate_df`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Select the feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Perform the transformation of the standardized data frame using the feature matrix and display the result. (Help: [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dot.html)) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform the transformation and display the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In this exercise, PCA allowed the two original attributes to be merged into one without any significant loss of information. Of course, it is very cumbersome to execute PCA step by step manually each time which is why PCA is also included in some ML frameworks such as scikit learn. \n",
    "\n",
    "It is very important to know exactly what the framework does for you and what it does not. For example, scikit-learn's PCA does not include standardization, although feature scaling [is strongly recommended](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) by the framework itself as a preceding step.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use scikit-learn's PCA to transform the standardized `currency_rate_df` a second time. This time you may assume that only one component is expected to be used as a result. Display the result. (Help: [Scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use PCA from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** It may be that the results of the manual PCA and scikit-learn's PCA differ. Explain why both results may well be correct. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
